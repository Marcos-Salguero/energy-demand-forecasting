{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOWNLOADING SCRIPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the other scripts or the user interface, it is essential to download the required data. The source used is: https://demanda.ree.es/visiona/peninsula/nacional/tablas/ \n",
    "\n",
    "Alternatively, the data can be manually downloaded from the files provided in the project. However, if data from a specific year is needed, simply execute the steps in this script and modify the corresponding year parameter.\n",
    "\n",
    "It is important to note that during the data extraction process, a manual adjustment is required for each year: identifying the date when daylight saving time was applied in Spain. Every year, the official time is moved forward by one hour on a specific date, which necessitates data interpolation to maintain temporal consistency and ensure complete records for all days of the year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start downloading all the data that we need, from 2020 to 2022 for training the model, and 2023 data for the testing. First, we download all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a loop to load the data for each year. We are going to do each year separately, because there are so many data, so I prefer to load it step by step, and have it saved independly. To load it, it is necessary to use web scraping, because all the data is posted in a url: https://demanda.ree.es/visiona/peninsula/nacional/tablas/.\n",
    "\n",
    "The data of each day have diferent url termination, so I'm going to use anidated loops in order to iterate for each combination of year, month and day.\n",
    "\n",
    "IMPORTANT NOTE: Currently the page has added some cookies, and this code won't work correctly. You can download manually the data from the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "columns = ['Real', 'Prevista', 'Programada', 'Date', 'Time']\n",
    "df_2023 = pd.DataFrame(columns=columns)\n",
    "\n",
    "# In order to download data for other years, you only need to change the range\n",
    "for year in range(2023, 2024):\n",
    "    for month in range(1, 13):\n",
    "        print(month)\n",
    "        for day in range(1, 32):\n",
    "\n",
    "            if (len(str(month)) == 1):\n",
    "                month = '0' + str(month)\n",
    "                \n",
    "            if (len(str(day)) == 1):\n",
    "                day = '0' + str(day)\n",
    "            url = 'https://demanda.ree.es/visiona/peninsula/nacional/tablas/'+ str(year) + '-' + str(month) + '-' + str(day) + '/1'\n",
    "\n",
    "            # First, we set the configurations for the chrome explorer to execute in headless mode (without opening explorer window)\n",
    "            chrome_options = Options()\n",
    "            chrome_options.add_argument('--headless')\n",
    "            chrome_options.page_load_strategy = 'normal'\n",
    "            driver = webdriver.Chrome(chrome_options)\n",
    "\n",
    "            driver.get(url)\n",
    "            table = WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, \"//table\")))\n",
    "\n",
    "            table_html = table.get_attribute('outerHTML')\n",
    "            driver.quit()\n",
    "            df = pd.read_html(table_html)[0]\n",
    "\n",
    "            if len(df) > 1:  # If the table data for that day exists\n",
    "                column_names = df.iloc[0]\n",
    "                df = df[1:]\n",
    "                df.columns = column_names\n",
    "                df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                # In the next steps we are only taking the values until 21:00, to do not save duplicated data\n",
    "                \n",
    "                df[['Date', 'Time']] = df['Hora'].str.split(' ', expand=True)\n",
    "                df['Date'] = pd.to_datetime(df['Date'])\n",
    "                df['Time'] = df['Time'].str.replace('2A', '02')\n",
    "                df = df[~df['Time'].str.contains('B')]\n",
    "                df['Time'] = pd.to_datetime(df['Time']).dt.time\n",
    "                df = df[0:288]\n",
    "                df.drop('Hora', axis=1, inplace=True)\n",
    "                df_2023 = pd.concat([df_2023, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to remove the duplicated data caused by the time change. Each year, on a specific date, the official time is set back by one hour, leading to duplicate records for that period. Therefore, it is essential to identify and remove these redundant entries to ensure data consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On the 29 March 2020\n",
    "df_2020 = df_2020.drop_duplicates()\n",
    "df_2020.to_csv('../data/df/df_2020.csv', index=False)\n",
    "# On the 28 March 2021\n",
    "df_2021 = df_2021.drop_duplicates()\n",
    "df_2021.to_csv('../data/df/df_2021.csv', index=False)\n",
    "# On the 26 March 2022\n",
    "df_2022 = df_2022.drop_duplicates()\n",
    "df_2022.to_csv('../data/df/df_2022.csv', index=False)\n",
    "# On the 26 March 2023\n",
    "df_2023 = df_2023.drop_duplicates()\n",
    "df_2023.to_csv('../data/df/df_2023.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will structure the dataset into the desired format. To achieve this, we will pivot the table, ensuring that each row represents a single day and contains the corresponding values for different time intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5min_1</th>\n",
       "      <th>5min_2</th>\n",
       "      <th>5min_3</th>\n",
       "      <th>5min_4</th>\n",
       "      <th>5min_5</th>\n",
       "      <th>5min_6</th>\n",
       "      <th>5min_7</th>\n",
       "      <th>5min_8</th>\n",
       "      <th>5min_9</th>\n",
       "      <th>5min_10</th>\n",
       "      <th>...</th>\n",
       "      <th>5min_279</th>\n",
       "      <th>5min_280</th>\n",
       "      <th>5min_281</th>\n",
       "      <th>5min_282</th>\n",
       "      <th>5min_283</th>\n",
       "      <th>5min_284</th>\n",
       "      <th>5min_285</th>\n",
       "      <th>5min_286</th>\n",
       "      <th>5min_287</th>\n",
       "      <th>5min_288</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-12-31</th>\n",
       "      <td>30.503</td>\n",
       "      <td>30.430</td>\n",
       "      <td>30.213</td>\n",
       "      <td>29.997</td>\n",
       "      <td>29.831</td>\n",
       "      <td>29.740</td>\n",
       "      <td>29.378</td>\n",
       "      <td>29.190</td>\n",
       "      <td>28.921</td>\n",
       "      <td>28.699</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-01</th>\n",
       "      <td>23.548</td>\n",
       "      <td>23.578</td>\n",
       "      <td>23.612</td>\n",
       "      <td>23.495</td>\n",
       "      <td>23.393</td>\n",
       "      <td>23.363</td>\n",
       "      <td>23.293</td>\n",
       "      <td>23.330</td>\n",
       "      <td>23.215</td>\n",
       "      <td>23.208</td>\n",
       "      <td>...</td>\n",
       "      <td>25.902</td>\n",
       "      <td>25.692</td>\n",
       "      <td>25.523</td>\n",
       "      <td>25.361</td>\n",
       "      <td>24.975</td>\n",
       "      <td>24.732</td>\n",
       "      <td>24.529</td>\n",
       "      <td>24.367</td>\n",
       "      <td>24.195</td>\n",
       "      <td>23.889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-02</th>\n",
       "      <td>23.720</td>\n",
       "      <td>23.690</td>\n",
       "      <td>23.543</td>\n",
       "      <td>23.349</td>\n",
       "      <td>23.079</td>\n",
       "      <td>22.909</td>\n",
       "      <td>22.833</td>\n",
       "      <td>22.680</td>\n",
       "      <td>22.386</td>\n",
       "      <td>22.279</td>\n",
       "      <td>...</td>\n",
       "      <td>29.527</td>\n",
       "      <td>29.410</td>\n",
       "      <td>29.136</td>\n",
       "      <td>28.848</td>\n",
       "      <td>28.430</td>\n",
       "      <td>28.324</td>\n",
       "      <td>28.024</td>\n",
       "      <td>27.809</td>\n",
       "      <td>27.408</td>\n",
       "      <td>27.289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-03</th>\n",
       "      <td>26.913</td>\n",
       "      <td>26.958</td>\n",
       "      <td>26.992</td>\n",
       "      <td>26.866</td>\n",
       "      <td>26.756</td>\n",
       "      <td>26.553</td>\n",
       "      <td>26.328</td>\n",
       "      <td>26.078</td>\n",
       "      <td>25.822</td>\n",
       "      <td>25.638</td>\n",
       "      <td>...</td>\n",
       "      <td>29.737</td>\n",
       "      <td>29.372</td>\n",
       "      <td>29.091</td>\n",
       "      <td>28.680</td>\n",
       "      <td>28.532</td>\n",
       "      <td>28.289</td>\n",
       "      <td>28.130</td>\n",
       "      <td>27.903</td>\n",
       "      <td>27.618</td>\n",
       "      <td>27.372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-04</th>\n",
       "      <td>27.222</td>\n",
       "      <td>27.302</td>\n",
       "      <td>27.195</td>\n",
       "      <td>26.762</td>\n",
       "      <td>26.504</td>\n",
       "      <td>26.425</td>\n",
       "      <td>26.288</td>\n",
       "      <td>26.207</td>\n",
       "      <td>25.926</td>\n",
       "      <td>25.897</td>\n",
       "      <td>...</td>\n",
       "      <td>29.411</td>\n",
       "      <td>29.262</td>\n",
       "      <td>29.137</td>\n",
       "      <td>28.804</td>\n",
       "      <td>28.543</td>\n",
       "      <td>28.361</td>\n",
       "      <td>28.143</td>\n",
       "      <td>27.828</td>\n",
       "      <td>27.574</td>\n",
       "      <td>27.314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 288 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            5min_1  5min_2  5min_3  5min_4  5min_5  5min_6  5min_7  5min_8  \\\n",
       "Date                                                                         \n",
       "2023-12-31  30.503  30.430  30.213  29.997  29.831  29.740  29.378  29.190   \n",
       "2024-01-01  23.548  23.578  23.612  23.495  23.393  23.363  23.293  23.330   \n",
       "2024-01-02  23.720  23.690  23.543  23.349  23.079  22.909  22.833  22.680   \n",
       "2024-01-03  26.913  26.958  26.992  26.866  26.756  26.553  26.328  26.078   \n",
       "2024-01-04  27.222  27.302  27.195  26.762  26.504  26.425  26.288  26.207   \n",
       "\n",
       "            5min_9  5min_10  ...  5min_279  5min_280  5min_281  5min_282  \\\n",
       "Date                         ...                                           \n",
       "2023-12-31  28.921   28.699  ...       NaN       NaN       NaN       NaN   \n",
       "2024-01-01  23.215   23.208  ...    25.902    25.692    25.523    25.361   \n",
       "2024-01-02  22.386   22.279  ...    29.527    29.410    29.136    28.848   \n",
       "2024-01-03  25.822   25.638  ...    29.737    29.372    29.091    28.680   \n",
       "2024-01-04  25.926   25.897  ...    29.411    29.262    29.137    28.804   \n",
       "\n",
       "            5min_283  5min_284  5min_285  5min_286  5min_287  5min_288  \n",
       "Date                                                                    \n",
       "2023-12-31       NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "2024-01-01    24.975    24.732    24.529    24.367    24.195    23.889  \n",
       "2024-01-02    28.430    28.324    28.024    27.809    27.408    27.289  \n",
       "2024-01-03    28.532    28.289    28.130    27.903    27.618    27.372  \n",
       "2024-01-04    28.543    28.361    28.143    27.828    27.574    27.314  \n",
       "\n",
       "[5 rows x 288 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2023 = pd.read_csv('../data/df/df_2023.csv')\n",
    "df_2023 = df_2023[df_2023['Date'] != '2022-12-31'].reset_index()\n",
    "df_2023 = df_2023.drop(['Programada', 'Prevista', 'index'], axis=1)\n",
    "\n",
    "# Ensure 'Date' is in datetime format\n",
    "df_2023['Date'] = pd.to_datetime(df_2023['Date'])\n",
    "\n",
    "# Generate a unique time interval identifier for each day\n",
    "# Assuming each day contains exactly 288 records (24 hours * 12 intervals per hour)\n",
    "df_2023['TimeID'] = (df_2023.groupby('Date').cumcount() + 1).astype(str)\n",
    "\n",
    "# Pivot the DataFrame to organize data by day\n",
    "df_pivoted_2023 = df_2023.pivot(index='Date', columns='TimeID', values='Real')\n",
    "\n",
    "# Rename columns to reflect time intervals\n",
    "df_pivoted_2023.columns = ['5min_' + str(col) for col in df_pivoted_2023.columns]\n",
    "\n",
    "sorted_columns = sorted(df_pivoted_2023.columns, key=lambda x: int(x.split('_')[1]))\n",
    "\n",
    "# Sort columns numerically\n",
    "df_pivoted_2023 = df_pivoted_2023[sorted_columns]\n",
    "\n",
    "# Display the first few rows of the transformed DataFrame\n",
    "df_pivoted_2023.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the table above, we can visually observe the new structure of the dataset after the pivoting process. This transformation is essential for preparing the data for model training.\n",
    "\n",
    "As previously mentioned, a manual adjustment is required each year to identify the date when the time change occurred (adding 1 hour). For instance, in Spain, this transition took place on March 26, 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to pay attention on the 2020-03-29, 2021-03-28, 2022-03-27 and 2023-03-26."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = df_pivoted_2023.index.get_loc('2023-03-26')\n",
    "for i in range(287, 35, -1):\n",
    "    df_pivoted_2023.iloc[indx, i] = df_pivoted_2023.iloc[indx, i-12]\n",
    "\n",
    "df_pivoted_2023.iloc[indx, 23:35] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the pivoted data locally. You will need to repeat the process with each year you want the data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivoted_2023.to_csv('../data/df_pivoted/df_pivoted_2023.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
