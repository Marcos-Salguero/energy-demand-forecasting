{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELECTRIC ENERGY DEMAND FORECASTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before delving into this exploration of Convolutional Neural Networks, it is recommended to first review the related material on Recurrent Neural Networks (file`2_programming_rnn.ipyn`). This sequence reflects the order in which these methods were explored, and references to the RNN model's results are made throughout this process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolucional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the project, we will explore the Convolutional Neural Network (CNN) method to observe how the prediction results behave using this approach.  \n",
    "We will begin by importing all the necessary libraries for this study.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to load the data that has already been downloaded and processed in the correct format for subsequent use in training and evaluating the models. If you do not yet have the data loaded, run the`downloading_script.ipynb` script or manually download the data from the`data` folder. Once you have the data locally, you can proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data and perform the necessary transformations for its subsequent use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivoted_2020 = pd.read_csv('../data/df_pivoted/df_pivoted_2020.csv')\n",
    "df_pivoted_2021 = pd.read_csv('../data/df_pivoted/df_pivoted_2021.csv')\n",
    "df_pivoted_2022 = pd.read_csv('../data/df_pivoted/df_pivoted_2022.csv')\n",
    "df_pivoted_2023 = pd.read_csv('../data/df_pivoted/df_pivoted_2023.csv')\n",
    "\n",
    "df_2021 = pd.read_csv('../data/df/df_2021.csv')\n",
    "df_2022 = pd.read_csv('../data/df/df_2022.csv')\n",
    "df_2023 = pd.read_csv('../data/df/df_2023.csv')\n",
    "\n",
    "# Updating specific columns for 2020 based on a specific date and applying interpolation for missing data\n",
    "indx = df_pivoted_2020[df_pivoted_2020['Date'] == '2020-03-29'].index[0]\n",
    "df_pivoted_2020.iloc[indx, 24:36] = [df_pivoted_2020.iloc[indx, 23] - i*(df_pivoted_2020.iloc[indx, 23] - df_pivoted_2020.iloc[indx, 36]) / 13 for i in range(1,13)]\n",
    "\n",
    "# Updating specific columns for 2021 based on a specific date and applying interpolation for missing data\n",
    "indx = df_pivoted_2021[df_pivoted_2021['Date'] == '2021-03-28'].index[0]\n",
    "df_pivoted_2021.iloc[indx, 24:36] = [df_pivoted_2021.iloc[indx, 23] - i*(df_pivoted_2021.iloc[indx, 23] - df_pivoted_2021.iloc[indx, 36]) / 13 for i in range(1,13)]\n",
    "\n",
    "# Updating specific columns for 2022 based on a specific date and applying interpolation for missing data\n",
    "indx = df_pivoted_2022[df_pivoted_2022['Date'] == '2022-03-27'].index[0]\n",
    "df_pivoted_2022.iloc[indx, 24:36] = [df_pivoted_2022.iloc[indx, 23] - i*(df_pivoted_2022.iloc[indx, 23] - df_pivoted_2022.iloc[indx, 36]) / 13 for i in range(1,13)]\n",
    "\n",
    "# Updating specific columns for 2023 based on a specific date and applying interpolation for missing data\n",
    "indx = df_pivoted_2023[df_pivoted_2023['Date'] == '2023-03-26'].index[0]\n",
    "df_pivoted_2023.iloc[indx, 24:36] = [df_pivoted_2023.iloc[indx, 23] - i*(df_pivoted_2023.iloc[indx, 23] - df_pivoted_2023.iloc[indx, 36]) / 13 for i in range(1,13)]\n",
    "\n",
    "# Handle missing values for 2020 by filling them with corresponding values from 2021 data\n",
    "null_index = df_pivoted_2020[df_pivoted_2020.isnull().any(axis=1)].index[0]\n",
    "df_pivoted_2020.iloc[null_index, 253:289] = df_2021[df_2021['Date'] == '2020-12-31']['Real']\n",
    "\n",
    "# Handle missing values for 2021 by filling them with corresponding values from 2022 data\n",
    "null_index = df_pivoted_2021[df_pivoted_2021.isnull().any(axis=1)].index[0]\n",
    "df_pivoted_2021.iloc[null_index, 253:289] = df_2022[df_2022['Date'] == '2021-12-31']['Real']\n",
    "\n",
    "# Handle missing values for 2022 by filling them with corresponding values from 2023 data\n",
    "null_index = df_pivoted_2022[df_pivoted_2022.isnull().any(axis=1)].index[0]\n",
    "df_pivoted_2022.iloc[null_index, 253:289] = df_2023[df_2023['Date'] == '2022-12-31']['Real']\n",
    "\n",
    "# Train dataframe creation\n",
    "train_df = pd.concat([df_pivoted_2020, df_pivoted_2021, df_pivoted_2022])\n",
    "train_df = train_df.reset_index().drop(['index'], axis=1)\n",
    "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
    "train_df['Day_Week'] = train_df['Date'].dt.day_name()\n",
    "\n",
    "# Test dataframe creation\n",
    "test_df = df_pivoted_2023.drop(df_pivoted_2023.index[-1])\n",
    "test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
    "test_df['Day_Week'] = test_df['Date'].dt.day_name()\n",
    "\n",
    "# Convert the 'Date' column to datetime type\n",
    "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
    "test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
    "\n",
    "# Encode the 'Day_Week' column into numeric variables\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['Day_Week'] = label_encoder.fit_transform(train_df['Day_Week'])\n",
    "test_df['Day_Week'] = label_encoder.fit_transform(test_df['Day_Week'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will scale our data, and here you have two versions of the dataset. One is the version I initially used for testing where I did not consider the day of the week data, and the other includes it (which is the version I ultimately used). I am providing both versions so you can experiment with them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_scaled = scaler.fit_transform(train_df.drop(['Date', 'Day_Week'], axis=1))\n",
    "test_scaled = scaler.fit_transform(test_df.drop(['Date', 'Day_Week'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_scaled = scaler.fit_transform(train_df.drop(['Date'], axis=1))\n",
    "test_scaled = scaler.fit_transform(test_df.drop(['Date'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, similar to the Recurrent Neural Networks, I need to create functions that allow me to reorder the data in the format required by the models, so that the pre-existing libraries can be utilized. In this case, the two versions I created are the same as with the RNN. Version 1 (which I use) takes the data from the last 21 days to predict day 22. Version 2, on the other hand, also includes the information from the same day of the week for the last 4 weeks. In practice, version 1 yielded the best results for me, but there is always the option to create new versions of the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset_v1(dataset, look_back=21):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back)]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back])\n",
    "\treturn np.array(dataX), np.array(dataY)\n",
    "\n",
    "def create_dataset_v2(dataset, look_back):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-29):\n",
    "\t\tmismo_dia = [(x * 7) + i for x in list(range(4))]\n",
    "\t\tuno = dataset[mismo_dia[-1]+7-look_back:mismo_dia[-1]+7]\n",
    "\t\tdos = dataset[mismo_dia]\n",
    "\t\ta = np.concatenate((uno,dos))\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[mismo_dia[-1]+7])\n",
    "\treturn np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use version 1 of the function to properly reorder my dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 21\n",
    "trainX, trainY = create_dataset_v1(train_scaled, look_back)\n",
    "testX, testY = create_dataset_v1(test_scaled, look_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will show you the structure your dataset should have. As you can see, the training dataset consists of 1074 records (corresponding to 3 years of training data), with 289 columns (representing the data collected for each day: 288 demand data points for every 5 minutes and 1 for the day of the week). Additionally, both the training and test sets have 21, representing the number of past days' data that the algorithm uses to predict the next day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train x shape: (1074, 21, 289)\n",
      "Test x shape: (342, 21, 289)\n",
      "Train y shape: (1074, 289)\n",
      "Test y shape: (342, 289)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train x shape:\", trainX.shape)\n",
    "print(\"Test x shape:\", testX.shape)\n",
    "\n",
    "print(\"Train y shape:\", trainY.shape)\n",
    "print(\"Test y shape:\", testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same TensorFlow library that we used for creating the RNN model. Let's define the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_4 (Conv1D)           (None, 21, 64)            18560     \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 7, 64)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 4, 64)             16448     \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 1, 64)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 289)               18785     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 57,953\n",
      "Trainable params: 57,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    # Convolutional layer with 64 filters and ReLU activation function\n",
    "    layers.Conv1D(64, kernel_size=1, activation='relu', input_shape=(21, 289)),\n",
    "    # Max pooling layer with a window size of 3\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    # Convolutional layer with 64 filters and ReLU activation function\n",
    "    layers.Conv1D(64, kernel_size=4, activation='relu'),\n",
    "    # Max pooling layer with a window size of 4\n",
    "    layers.MaxPooling1D(pool_size=4),\n",
    "    # Flatten layer to connect with fully connected layers\n",
    "    layers.Flatten(),\n",
    "    # Fully connected layer with 64 neurons and ReLU activation function\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    # Output layer with 289 neurons\n",
    "    layers.Dense(289)\n",
    "])\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is a Convolutional Neural Network (CNN) designed to work with multivariate time series data. Below is an explanation of each layer:\n",
    "\n",
    "**Conv1D Layer**: This is the first convolutional layer. Conv1D refers to a one-dimensional convolution, which is suitable for time series data. It has 64 filters, meaning the layer will learn 64 different patterns from the input data. Each filter has a size of 1 (kernel_size), meaning each time the filter is applied, one consecutive time point will be considered. The ReLU activation function is applied after the convolution. The input shape specified is (21, 289), meaning each sample has a length of 21 time points, and each time point has 289 features (variables).\n",
    "\n",
    "**MaxPooling1D Layer**: This is a one-dimensional max pooling layer. MaxPooling1D is used to reduce the dimensionality of the output from the convolutional layer while preserving the most important features. Here, a max pooling operation with a window size of 3 is applied, reducing the length of the resulting sequence by one-third.\n",
    "\n",
    "**Second Conv1D Layer**: This is another convolutional layer similar to the first one, with 64 filters.\n",
    "\n",
    "**Second MaxPooling1D Layer**: Another max pooling layer that further reduces the dimensionality of the output.\n",
    "\n",
    "**Flatten Layer**: This layer is used to flatten the output from the convolutional layers and prepare it to be fed into the fully connected layers.\n",
    "\n",
    "**Dense Layer**: This is a fully connected layer with 64 neurons and the ReLU activation function. Dense layers are used to learn non-linear relationships in the data.\n",
    "\n",
    "**Dense Output Layer**: This is the final output layer. It has 289 neurons, matching the number of features in your output data. This layer does not have a specific activation function, meaning the output is not restricted to any particular range.\n",
    "\n",
    "We now train the model for 50 epochs with a batch_size of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20fbe9ddd50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, epochs=50, batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will evaluate the model's performance using the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 1s 9ms/step\n",
      "11/11 [==============================] - 0s 9ms/step\n",
      "Train Score: 1.39 RMSE\n",
      "Test Score: 2.29 RMSE\n"
     ]
    }
   ],
   "source": [
    "trainY_inv = scaler.inverse_transform(trainY)\n",
    "testY_inv = scaler.inverse_transform(testY)\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY_inv, trainPredict))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY_inv, testPredict))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these parameters, we obtain an RMSE of 1.39 GW on the test set. However, with the evaluation set, we get an RMSE of 2.29, which is not very good considering the results achieved with the RNN model. \n",
    "\n",
    "Next, just as we did with the RNN model, we will use nested loops to test different combinations of parameters such as the number of neurons and the number of epochs. We will store all the results in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "  Neurons Epochs  Train RMSE  Test RMSE\n",
      "0      20     20    1.464021   1.886013\n",
      "34/34 [==============================] - 0s 4ms/step\n",
      "11/11 [==============================] - 0s 4ms/step\n",
      "  Neurons Epochs  Train RMSE  Test RMSE\n",
      "0      20     20    1.464021   1.886013\n",
      "0      20     50    1.393362   1.914653\n",
      "34/34 [==============================] - 0s 3ms/step\n",
      "11/11 [==============================] - 0s 3ms/step\n",
      "  Neurons Epochs  Train RMSE  Test RMSE\n",
      "0      20     20    1.464021   1.886013\n",
      "0      20     50    1.393362   1.914653\n",
      "0      20    100    1.294735   1.961142\n",
      "34/34 [==============================] - 1s 8ms/step\n",
      "11/11 [==============================] - 0s 7ms/step\n",
      "  Neurons Epochs  Train RMSE  Test RMSE\n",
      "0      20     20    1.464021   1.886013\n",
      "0      20     50    1.393362   1.914653\n",
      "0      20    100    1.294735   1.961142\n",
      "0      50     20    1.801341   2.127014\n",
      "34/34 [==============================] - 0s 5ms/step\n",
      "11/11 [==============================] - 0s 4ms/step\n",
      "  Neurons Epochs  Train RMSE  Test RMSE\n",
      "0      20     20    1.464021   1.886013\n",
      "0      20     50    1.393362   1.914653\n",
      "0      20    100    1.294735   1.961142\n",
      "0      50     20    1.801341   2.127014\n",
      "0      50     50    1.254227   1.731056\n",
      "34/34 [==============================] - 0s 3ms/step\n",
      "11/11 [==============================] - 0s 4ms/step\n",
      "  Neurons Epochs  Train RMSE  Test RMSE\n",
      "0      20     20    1.464021   1.886013\n",
      "0      20     50    1.393362   1.914653\n",
      "0      20    100    1.294735   1.961142\n",
      "0      50     20    1.801341   2.127014\n",
      "0      50     50    1.254227   1.731056\n",
      "0      50    100    1.198228   1.865100\n",
      "34/34 [==============================] - 0s 8ms/step\n",
      "11/11 [==============================] - 0s 6ms/step\n",
      "  Neurons Epochs  Train RMSE  Test RMSE\n",
      "0      20     20    1.464021   1.886013\n",
      "0      20     50    1.393362   1.914653\n",
      "0      20    100    1.294735   1.961142\n",
      "0      50     20    1.801341   2.127014\n",
      "0      50     50    1.254227   1.731056\n",
      "0      50    100    1.198228   1.865100\n",
      "0     100     20    1.449771   1.950946\n",
      "34/34 [==============================] - 0s 5ms/step\n",
      "11/11 [==============================] - 0s 5ms/step\n",
      "  Neurons Epochs  Train RMSE  Test RMSE\n",
      "0      20     20    1.464021   1.886013\n",
      "0      20     50    1.393362   1.914653\n",
      "0      20    100    1.294735   1.961142\n",
      "0      50     20    1.801341   2.127014\n",
      "0      50     50    1.254227   1.731056\n",
      "0      50    100    1.198228   1.865100\n",
      "0     100     20    1.449771   1.950946\n",
      "0     100     50    1.379085   1.807659\n",
      "34/34 [==============================] - 0s 6ms/step\n",
      "11/11 [==============================] - 0s 6ms/step\n",
      "  Neurons Epochs  Train RMSE  Test RMSE\n",
      "0      20     20    1.464021   1.886013\n",
      "0      20     50    1.393362   1.914653\n",
      "0      20    100    1.294735   1.961142\n",
      "0      50     20    1.801341   2.127014\n",
      "0      50     50    1.254227   1.731056\n",
      "0      50    100    1.198228   1.865100\n",
      "0     100     20    1.449771   1.950946\n",
      "0     100     50    1.379085   1.807659\n",
      "0     100    100    1.355058   2.121654\n"
     ]
    }
   ],
   "source": [
    "df_nn = pd.DataFrame(columns=['Neurons', 'Epochs', 'Train RMSE', 'Test RMSE'])\n",
    "trainY_inv = scaler.inverse_transform(trainY)\n",
    "testY_inv = scaler.inverse_transform(testY)\n",
    "\n",
    "for neurons_number in [20, 50, 100]:\n",
    "    for num_epochs in [20, 50, 100]:\n",
    "\n",
    "        model = models.Sequential([\n",
    "            layers.Conv1D(neurons_number, kernel_size=6, activation='relu', input_shape=(21, 289)),\n",
    "            layers.MaxPooling1D(pool_size=4),\n",
    "            layers.Conv1D(64, kernel_size=2, activation='relu'),\n",
    "            layers.MaxPooling1D(pool_size=3),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(289)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        model.fit(trainX, trainY, epochs=num_epochs, batch_size=1, verbose=0)\n",
    "\n",
    "        # make predictions\n",
    "        trainPredict = model.predict(trainX)\n",
    "        testPredict = model.predict(testX)\n",
    "\n",
    "        \n",
    "        # invert predictions\n",
    "        trainPredict = scaler.inverse_transform(trainPredict)\n",
    "        testPredict = scaler.inverse_transform(testPredict)\n",
    "        \n",
    "        # calculate root mean squared error\n",
    "        trainScore = math.sqrt(mean_squared_error(trainY_inv, trainPredict))\n",
    "        testScore = math.sqrt(mean_squared_error(testY_inv, testPredict))\n",
    "\n",
    "        new_row = {'Neurons': neurons_number, 'Epochs': num_epochs, 'Train RMSE': trainScore, 'Test RMSE': testScore}\n",
    "        df_nn = pd.concat([df_nn, pd.DataFrame([new_row])])\n",
    "        print(df_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the results improve compared to the initial test we conducted, but they are still worse than those obtained with the RNN model.\n",
    "\n",
    "During the time I was searching for the model that would yield the best solution, I also experimented, as mentioned, with different dataset sizes. For example, this case is the evaluation using the data from the last 27 days (4 weeks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 2s 20ms/step\n",
      "11/11 [==============================] - 0s 7ms/step\n",
      "  Neurons Epochs  Train RMSE  Test RMSE\n",
      "0      20     20    1.547657   1.963241\n",
      "33/33 [==============================] - 0s 6ms/step\n",
      "11/11 [==============================] - 0s 7ms/step\n",
      "  Neurons Epochs  Train RMSE  Test RMSE\n",
      "0      20     20    1.547657   1.963241\n",
      "0      20     50    1.402327   2.002637\n",
      "33/33 [==============================] - 0s 6ms/step\n",
      "11/11 [==============================] - 0s 6ms/step\n",
      "  Neurons Epochs  Train RMSE  Test RMSE\n",
      "0      20     20    1.547657   1.963241\n",
      "0      20     50    1.402327   2.002637\n",
      "0      20    100    1.351819   2.223125\n",
      "33/33 [==============================] - 1s 7ms/step\n",
      "11/11 [==============================] - 0s 6ms/step\n",
      "  Neurons Epochs  Train RMSE  Test RMSE\n",
      "0      20     20    1.547657   1.963241\n",
      "0      20     50    1.402327   2.002637\n",
      "0      20    100    1.351819   2.223125\n",
      "0      50     20    1.268707   1.785111\n",
      "33/33 [==============================] - 0s 7ms/step\n",
      "11/11 [==============================] - 0s 6ms/step\n",
      "  Neurons Epochs  Train RMSE  Test RMSE\n",
      "0      20     20    1.547657   1.963241\n",
      "0      20     50    1.402327   2.002637\n",
      "0      20    100    1.351819   2.223125\n",
      "0      50     20    1.268707   1.785111\n",
      "0      50     50    1.098674   1.698830\n",
      "33/33 [==============================] - 1s 7ms/step\n",
      "11/11 [==============================] - 0s 7ms/step\n",
      "  Neurons Epochs  Train RMSE  Test RMSE\n",
      "0      20     20    1.547657   1.963241\n",
      "0      20     50    1.402327   2.002637\n",
      "0      20    100    1.351819   2.223125\n",
      "0      50     20    1.268707   1.785111\n",
      "0      50     50    1.098674   1.698830\n",
      "0      50    100    1.391993   1.881022\n",
      "33/33 [==============================] - 1s 8ms/step\n",
      "11/11 [==============================] - 0s 8ms/step\n",
      "  Neurons Epochs  Train RMSE  Test RMSE\n",
      "0      20     20    1.547657   1.963241\n",
      "0      20     50    1.402327   2.002637\n",
      "0      20    100    1.351819   2.223125\n",
      "0      50     20    1.268707   1.785111\n",
      "0      50     50    1.098674   1.698830\n",
      "0      50    100    1.391993   1.881022\n",
      "0     100     20    1.330036   1.773012\n",
      "33/33 [==============================] - 0s 7ms/step\n",
      "11/11 [==============================] - 0s 6ms/step\n",
      "  Neurons Epochs  Train RMSE  Test RMSE\n",
      "0      20     20    1.547657   1.963241\n",
      "0      20     50    1.402327   2.002637\n",
      "0      20    100    1.351819   2.223125\n",
      "0      50     20    1.268707   1.785111\n",
      "0      50     50    1.098674   1.698830\n",
      "0      50    100    1.391993   1.881022\n",
      "0     100     20    1.330036   1.773012\n",
      "0     100     50    1.286420   1.648811\n",
      "33/33 [==============================] - 1s 11ms/step\n",
      "11/11 [==============================] - 0s 17ms/step\n",
      "  Neurons Epochs  Train RMSE  Test RMSE\n",
      "0      20     20    1.547657   1.963241\n",
      "0      20     50    1.402327   2.002637\n",
      "0      20    100    1.351819   2.223125\n",
      "0      50     20    1.268707   1.785111\n",
      "0      50     50    1.098674   1.698830\n",
      "0      50    100    1.391993   1.881022\n",
      "0     100     20    1.330036   1.773012\n",
      "0     100     50    1.286420   1.648811\n",
      "0     100    100    1.037903   1.595321\n"
     ]
    }
   ],
   "source": [
    "df_nn = pd.DataFrame(columns=['Neurons', 'Epochs', 'Train RMSE', 'Test RMSE'])\n",
    "trainY_inv = scaler.inverse_transform(trainY)\n",
    "testY_inv = scaler.inverse_transform(testY)\n",
    "\n",
    "for neurons_number in [20, 50, 100]:\n",
    "    for num_epochs in [20, 50, 100]:\n",
    "\n",
    "        model = models.Sequential([\n",
    "            layers.Conv1D(neurons_number, kernel_size=7, activation='relu', input_shape=(27, 289)),\n",
    "            layers.MaxPooling1D(pool_size=3),\n",
    "            layers.Conv1D(64, kernel_size=4, activation='relu'),\n",
    "            layers.MaxPooling1D(pool_size=4),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(289)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        model.fit(trainX, trainY, epochs=num_epochs, batch_size=1, verbose=0)\n",
    "\n",
    "        # make predictions\n",
    "        trainPredict = model.predict(trainX)\n",
    "        testPredict = model.predict(testX)\n",
    "\n",
    "        \n",
    "        # invert predictions\n",
    "        trainPredict = scaler.inverse_transform(trainPredict)\n",
    "        testPredict = scaler.inverse_transform(testPredict)\n",
    "        \n",
    "        # calculate root mean squared error\n",
    "        trainScore = math.sqrt(mean_squared_error(trainY_inv, trainPredict))\n",
    "        testScore = math.sqrt(mean_squared_error(testY_inv, testPredict))\n",
    "\n",
    "        new_row = {'Neurons': neurons_number, 'Epochs': num_epochs, 'Train RMSE': trainScore, 'Test RMSE': testScore}\n",
    "        df_nn = pd.concat([df_nn, pd.DataFrame([new_row])])\n",
    "        print(df_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the results are very close but the difficulty of the model increases to a greater extent, I decided to stick with the option of using 21 days. The next step is to find the optimal value for the number of neurons. To do this, we will use the GridSearchCV function, testing different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msalg\\AppData\\Local\\Temp\\ipykernel_21424\\1691729805.py:20: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasRegressor(build_fn=create_cnn, epochs=100, batch_size=1, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor resultado: -0.007537 usando {'neurons_number': 64}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras import models, layers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Function to create the CNN model\n",
    "def create_cnn(neurons_number=64):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv1D(neurons_number, kernel_size=6, activation='relu', input_shape=(21, 289)),\n",
    "        layers.MaxPooling1D(pool_size=4),\n",
    "        layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(289)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Create the KerasRegressor model\n",
    "model = KerasRegressor(build_fn=create_cnn, epochs=100, batch_size=1, verbose=0)\n",
    "\n",
    "# Define the hyperparameters for grid search\n",
    "param_grid = {'neurons_number': [32, 64, 128]}  # You can adjust the values as needed\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_result = grid_search.fit(trainX, trainY)\n",
    "\n",
    "# Display the results\n",
    "print(\"Best result: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the optimal number of neurons is 64. Therefore, I will create and evaluate a model with these parameters to compare it with the best model obtained in the RNN study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 5ms/step\n",
      "11/11 [==============================] - 0s 4ms/step\n",
      "Train Score: 1.26 RMSE\n",
      "Test Score: 2.26 RMSE\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv1D(64, kernel_size=6, activation='relu', input_shape=(21, 289)),\n",
    "    layers.MaxPooling1D(pool_size=4),\n",
    "    layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(289)\n",
    "])\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=0)\n",
    "\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "\n",
    "trainScore = math.sqrt(mean_squared_error(trainY_inv, trainPredict))\n",
    "testScore = math.sqrt(mean_squared_error(testY_inv, testPredict))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will test with different values of kernel_size and pool_size to see if it changes the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 3ms/step\n",
      "Train Score: 1.56 RMSE\n",
      "Test Score: 2.27 RMSE\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv1D(64, kernel_size=1, activation='relu', input_shape=(21, 289)),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(64, kernel_size=4, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=4),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(289)\n",
    "])\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=0)\n",
    "\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "\n",
    "trainScore = math.sqrt(mean_squared_error(trainY_inv, trainPredict))\n",
    "testScore = math.sqrt(mean_squared_error(testY_inv, testPredict))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, I will not continue exploring this path, as the results obtained have not been promising. After analyzing the potential of RNNs and CNNs, it is evident from the results that RNNs are better at handling and predicting time series data. CNNs, on the other hand, are not able to capture this relationship effectively, which is why their results are considerably worse than those of RNNs.\n",
    "\n",
    "However, there is still much more to explore in this area. It is possible to optimize many more parameters (not just the number of neurons) as well as all the other parameters used in the model. One can also experiment with modifying the input dataset, both by altering its format (such as adding more than 21 days of data to predict day number 22) and by including additional fields of interest (for example, weather data)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
